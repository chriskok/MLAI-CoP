{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification Tutorial \n",
    "Credit to: https://stackabuse.com/text-classification-with-python-and-scikit-learn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.datasets import load_files\n",
    "import pickle\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset: Cornell Natural Language Processing Group. The dataset consists of a total of 2000 documents\n",
    "\n",
    "Check out what the data looks like in the review_polarity/txt_sentoken directory. Particularly cv001 in NEG reviews; it contains words like GOOD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_data = load_files(r\"../review_polarity/txt_sentoken\")\n",
    "X, y = movie_data.data, movie_data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print the length of the array \"X\" below and notice that we start with 2000 documents read. On the other hand, \"y\" contains all the labels (either negative or positive based on the folders they were in 'neg' or 'pos'). \n",
    "\n",
    "Remember that someone had to go through all these reviews and label them into these two folders manually. They read through all these reviews and placed them nicely in folders for us based on their sentiment accordingly.\n",
    "\n",
    "Please note that these are case sensitive when programming with them later on. \n",
    "\n",
    "**Notes to self**: Remind the audience here that jupyter notebook is sequential (which is why there are numbers by it's side), so we can't skip ahead for example (going to the next block printing X before the previous of reading X). \n",
    "\n",
    "Also, if they ever get in trouble and aren't sure where they are, just refresh the notebook and run all. there's also a completed notebook in the /completed/ folder that you can follow if you're newer to all this (show them where to get it)\n",
    "\n",
    "Lastly, slow down. You'll be fine :) \n",
    "\n",
    "**Disclaimer**: Binder is a free tool that's publically available and we highly suggest not using it with any proprietary data - unsure about the security plus you may lose data when logging out. There are plenty of tools and people around the company who can assist you with any of these needs if you're interested in using it for actual projects (just reach out!). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing out the first review in X just to see the data we've pulled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"arnold schwarzenegger has been an icon for action enthusiasts , since the late 80's , but lately his films have been very sloppy and the one-liners are getting worse . \\nit's hard seeing arnold as mr . freeze in batman and robin , especially when he says tons of ice jokes , but hey he got 15 million , what's it matter to him ? \\nonce again arnold has signed to do another expensive blockbuster , that can't compare with the likes of the terminator series , true lies and even eraser . \\nin this so called dark thriller , the devil ( gabriel byrne ) has come upon earth , to impregnate a woman ( robin tunney ) which happens every 1000 years , and basically destroy the world , but apparently god has chosen one man , and that one man is jericho cane ( arnold himself ) . \\nwith the help of a trusty sidekick ( kevin pollack ) , they will stop at nothing to let the devil take over the world ! \\nparts of this are actually so absurd , that they would fit right in with dogma . \\nyes , the film is that weak , but it's better than the other blockbuster right now ( sleepy hollow ) , but it makes the world is not enough look like a 4 star film . \\nanyway , this definitely doesn't seem like an arnold movie . \\nit just wasn't the type of film you can see him doing . \\nsure he gave us a few chuckles with his well known one-liners , but he seemed confused as to where his character and the film was going . \\nit's understandable , especially when the ending had to be changed according to some sources . \\naside form that , he still walked through it , much like he has in the past few films . \\ni'm sorry to say this arnold but maybe these are the end of your action days . \\nspeaking of action , where was it in this film ? \\nthere was hardly any explosions or fights . \\nthe devil made a few places explode , but arnold wasn't kicking some devil butt . \\nthe ending was changed to make it more spiritual , which undoubtedly ruined the film . \\ni was at least hoping for a cool ending if nothing else occurred , but once again i was let down . \\ni also don't know why the film took so long and cost so much . \\nthere was really no super affects at all , unless you consider an invisible devil , who was in it for 5 minutes tops , worth the overpriced budget . \\nthe budget should have gone into a better script , where at least audiences could be somewhat entertained instead of facing boredom . \\nit's pitiful to see how scripts like these get bought and made into a movie . \\ndo they even read these things anymore ? \\nit sure doesn't seem like it . \\nthankfully gabriel's performance gave some light to this poor film . \\nwhen he walks down the street searching for robin tunney , you can't help but feel that he looked like a devil . \\nthe guy is creepy looking anyway ! \\nwhen it's all over , you're just glad it's the end of the movie . \\ndon't bother to see this , if you're expecting a solid action flick , because it's neither solid nor does it have action . \\nit's just another movie that we are suckered in to seeing , due to a strategic marketing campaign . \\nsave your money and see the world is not enough for an entertaining experience . \\n\"\n"
     ]
    }
   ],
   "source": [
    "print(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y labels: 0 is NEGATIVE, 1 is POSITIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "for sen in range(0, len(X)):\n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\W', ' ', str(X[sen]))\n",
    "    \n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "    \n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    \n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "    \n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "    \n",
    "    # Lemmatization\n",
    "    document = document.split()\n",
    "\n",
    "    document = [stemmer.lemmatize(word) for word in document]\n",
    "    document = ' '.join(document)\n",
    "    \n",
    "    documents.append(document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b\"arnold schwarzenegger has been an icon for action enthusiasts , since the late 80's , but lately his films have been very sloppy and the one-liners are getting worse . \\nit's hard seeing arnold as mr . freeze in batman and robin , especially when he says tons of ice jokes , but hey he got 15 million , what's it matter to him ? \\nonce again arnold has signed to do another expensive blockbuster , that can't compare with the likes of the terminator series , true lies and even eraser . \\nin this so called dark thriller , the devil ( gabriel byrne ) has come upon earth , to impregnate a woman ( robin tunney ) which happens every 1000 years , and basically destroy the world , but apparently god has chosen one man , and that one man is jericho cane ( arnold himself ) . \\nwith the help of a trusty sidekick ( kevin pollack ) , they will stop at nothing to let the devil take over the world ! \\nparts of this are actually so absurd , that they would fit right in with dogma . \\nyes , the film is that weak , but it's better than the other blockbuster right now ( sleepy hollow ) , but it makes the world is not enough look like a 4 star film . \\nanyway , this definitely doesn't seem like an arnold movie . \\nit just wasn't the type of film you can see him doing . \\nsure he gave us a few chuckles with his well known one-liners , but he seemed confused as to where his character and the film was going . \\nit's understandable , especially when the ending had to be changed according to some sources . \\naside form that , he still walked through it , much like he has in the past few films . \\ni'm sorry to say this arnold but maybe these are the end of your action days . \\nspeaking of action , where was it in this film ? \\nthere was hardly any explosions or fights . \\nthe devil made a few places explode , but arnold wasn't kicking some devil butt . \\nthe ending was changed to make it more spiritual , which undoubtedly ruined the film . \\ni was at least hoping for a cool ending if nothing else occurred , but once again i was let down . \\ni also don't know why the film took so long and cost so much . \\nthere was really no super affects at all , unless you consider an invisible devil , who was in it for 5 minutes tops , worth the overpriced budget . \\nthe budget should have gone into a better script , where at least audiences could be somewhat entertained instead of facing boredom . \\nit's pitiful to see how scripts like these get bought and made into a movie . \\ndo they even read these things anymore ? \\nit sure doesn't seem like it . \\nthankfully gabriel's performance gave some light to this poor film . \\nwhen he walks down the street searching for robin tunney , you can't help but feel that he looked like a devil . \\nthe guy is creepy looking anyway ! \\nwhen it's all over , you're just glad it's the end of the movie . \\ndon't bother to see this , if you're expecting a solid action flick , because it's neither solid nor does it have action . \\nit's just another movie that we are suckered in to seeing , due to a strategic marketing campaign . \\nsave your money and see the world is not enough for an entertaining experience . \\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arnold schwarzenegger ha been an icon for action enthusiast since the late 80 but lately his film have been very sloppy and the one liner are getting worse nit hard seeing arnold a mr freeze in batman and robin especially when he say ton of ice joke but hey he got 15 million what it matter to him nonce again arnold ha signed to do another expensive blockbuster that can compare with the like of the terminator series true lie and even eraser nin this so called dark thriller the devil gabriel byrne ha come upon earth to impregnate woman robin tunney which happens every 1000 year and basically destroy the world but apparently god ha chosen one man and that one man is jericho cane arnold himself nwith the help of trusty sidekick kevin pollack they will stop at nothing to let the devil take over the world nparts of this are actually so absurd that they would fit right in with dogma nyes the film is that weak but it better than the other blockbuster right now sleepy hollow but it make the world is not enough look like 4 star film nanyway this definitely doesn seem like an arnold movie nit just wasn the type of film you can see him doing nsure he gave u few chuckle with his well known one liner but he seemed confused a to where his character and the film wa going nit understandable especially when the ending had to be changed according to some source naside form that he still walked through it much like he ha in the past few film ni sorry to say this arnold but maybe these are the end of your action day nspeaking of action where wa it in this film nthere wa hardly any explosion or fight nthe devil made few place explode but arnold wasn kicking some devil butt nthe ending wa changed to make it more spiritual which undoubtedly ruined the film ni wa at least hoping for cool ending if nothing else occurred but once again wa let down ni also don know why the film took so long and cost so much nthere wa really no super affect at all unless you consider an invisible devil who wa in it for 5 minute top worth the overpriced budget nthe budget should have gone into better script where at least audience could be somewhat entertained instead of facing boredom nit pitiful to see how script like these get bought and made into movie ndo they even read these thing anymore nit sure doesn seem like it nthankfully gabriel performance gave some light to this poor film nwhen he walk down the street searching for robin tunney you can help but feel that he looked like devil nthe guy is creepy looking anyway nwhen it all over you re just glad it the end of the movie ndon bother to see this if you re expecting solid action flick because it neither solid nor doe it have action nit just another movie that we are suckered in to seeing due to strategic marketing campaign nsave your money and see the world is not enough for an entertaining experience'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^ cuts down all the noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other measures of NLP preprocessing: n-grams, stopwords, lemmatize conversion vs stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "X1 = vectorizer.fit_transform(documents).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer makes very unique word a feature for the ml model. Parameters:\n",
    "\n",
    "- We take a max of 1500 features (the most common, highest occuring 1500 unique words)\n",
    "- min_df is number of documents they appear in at minimum\n",
    "- max_df is the maximum percentage of documents containing this word\n",
    "- Finally, stop_words removes any very commons words in the english language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000', '10', '13', '1997', '1998', '1999', '20', '80', '90', 'ability', 'able', 'absolutely', 'academy', 'accent', 'accident', 'across', 'act', 'acting', 'action', 'actor', 'actress', 'actual', 'actually', 'adam', 'adaptation', 'add', 'addition', 'admit', 'adult', 'adventure', 'affair', 'affleck', 'age', 'agent', 'ago', 'air', 'alan', 'alien', 'alive', 'allen', 'allows', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'amazing', 'america', 'american', 'among', 'amount', 'amusing', 'anderson', 'angel', 'angle', 'angry', 'animal', 'animated', 'animation', 'annoying', 'another', 'answer', 'anti', 'anyone', 'anything', 'anyway', 'apart', 'apartment', 'ape', 'apparent', 'apparently', 'appeal', 'appealing', 'appear', 'appearance', 'appears', 'appreciate', 'approach', 'appropriate', 'arm', 'army', 'around', 'art', 'artist', 'aside', 'ask', 'asked', 'asks', 'aspect', 'atmosphere', 'attack', 'attempt', 'attention', 'attitude', 'audience', 'average', 'award', 'away', 'awful', 'baby', 'back', 'background', 'bad', 'ball', 'band', 'bank', 'bar', 'barely', 'based', 'basic', 'basically', 'batman', 'battle', 'beat', 'beautiful', 'beauty', 'became', 'become', 'becomes', 'becoming', 'begin', 'beginning', 'behind', 'belief', 'believable', 'believe', 'ben', 'best', 'better', 'beyond', 'big', 'biggest', 'bill', 'billy', 'bit', 'bizarre', 'black', 'blade', 'blair', 'blame', 'blood', 'blow', 'blue', 'board', 'boat', 'bob', 'body', 'bond', 'book', 'boring', 'born', 'bos', 'box', 'boy', 'boyfriend', 'brain', 'break', 'brief', 'bright', 'brilliant', 'bring', 'brings', 'british', 'brother', 'brought', 'brown', 'bruce', 'buddy', 'budget', 'bug', 'build', 'building', 'bunch', 'burton', 'business', 'buy', 'cage', 'call', 'called', 'came', 'cameo', 'camera', 'cameron', 'camp', 'campbell', 'cannot', 'captain', 'capture', 'car', 'care', 'career', 'carrey', 'carry', 'carter', 'cartoon', 'case', 'cash', 'cast', 'casting', 'catch', 'caught', 'cause', 'center', 'central', 'century', 'certain', 'certainly', 'chan', 'chance', 'change', 'characterization', 'charm', 'charming', 'chase', 'cheap', 'chemistry', 'child', 'chinese', 'choice', 'chris', 'christmas', 'christopher', 'cinema', 'cinematic', 'cinematography', 'city', 'claim', 'class', 'classic', 'clear', 'clearly', 'clever', 'cliche', 'climax', 'close', 'club', 'co', 'cold', 'college', 'color', 'come', 'comedic', 'comedy', 'comic', 'coming', 'commercial', 'common', 'company', 'comparison', 'complete', 'completely', 'complex', 'computer', 'concept', 'conclusion', 'conflict', 'confused', 'confusing', 'consider', 'considered', 'considering', 'constantly', 'contact', 'contains', 'continues', 'control', 'convincing', 'cool', 'cop', 'costume', 'could', 'count', 'country', 'couple', 'course', 'cover', 'crap', 'crash', 'crazy', 'create', 'created', 'creates', 'creating', 'creature', 'credit', 'creepy', 'crew', 'crime', 'criminal', 'critic', 'cross', 'crowd', 'cry', 'culture', 'cut', 'cute', 'dance', 'danny', 'dark', 'date', 'daughter', 'david', 'day', 'de', 'dead', 'deal', 'death', 'debut', 'decade', 'decent', 'decide', 'decided', 'decides', 'decision', 'deep', 'definitely', 'deliver', 'delivers', 'depth', 'deserves', 'design', 'desire', 'desperate', 'despite', 'detail', 'detective', 'developed', 'development', 'device', 'devil', 'dialogue', 'die', 'difference', 'different', 'difficult', 'dimensional', 'directed', 'directing', 'direction', 'director', 'disappointing', 'disaster', 'discover', 'discovers', 'disney', 'display', 'disturbing', 'doctor', 'documentary', 'doe', 'dog', 'dollar', 'done', 'door', 'double', 'doubt', 'douglas', 'dozen', 'dr', 'drag', 'drama', 'dramatic', 'draw', 'dream', 'drive', 'driver', 'drug', 'dude', 'due', 'dull', 'dumb', 'dy', 'earlier', 'early', 'earth', 'easily', 'easy', 'ed', 'eddie', 'edge', 'editing', 'edward', 'effect', 'effective', 'effort', 'either', 'element', 'elizabeth', 'else', 'emotion', 'emotional', 'encounter', 'end', 'ending', 'energy', 'english', 'enjoy', 'enjoyable', 'enjoyed', 'enough', 'entertaining', 'entertainment', 'entire', 'entirely', 'epic', 'episode', 'equally', 'era', 'escape', 'especially', 'etc', 'even', 'event', 'eventually', 'ever', 'every', 'everyone', 'everything', 'evil', 'ex', 'exactly', 'example', 'excellent', 'except', 'exception', 'exciting', 'expect', 'expectation', 'expected', 'experience', 'explain', 'explosion', 'extra', 'extremely', 'eye', 'face', 'fact', 'failed', 'fails', 'failure', 'fair', 'fairly', 'fake', 'fall', 'familiar', 'family', 'famous', 'fan', 'fantasy', 'far', 'fare', 'fascinating', 'fashion', 'fast', 'fate', 'father', 'fault', 'favorite', 'fbi', 'fear', 'feature', 'feel', 'feeling', 'fellow', 'felt', 'female', 'fi', 'fiction', 'field', 'fight', 'fighting', 'figure', 'fill', 'filled', 'filmed', 'filmmaker', 'filmmaking', 'final', 'finally', 'find', 'fine', 'fire', 'first', 'fit', 'five', 'flashback', 'flat', 'flaw', 'flick', 'floor', 'fly', 'flying', 'focus', 'folk', 'follow', 'following', 'follows', 'food', 'foot', 'footage', 'force', 'forced', 'forever', 'forget', 'form', 'former', 'formula', 'forward', 'found', 'four', 'fox', 'frame', 'frank', 'free', 'french', 'fresh', 'friend', 'front', 'full', 'fully', 'fun', 'funny', 'future', 'gag', 'game', 'gang', 'gangster', 'gave', 'gay', 'general', 'generally', 'generation', 'genius', 'genre', 'genuine', 'george', 'get', 'getting', 'ghost', 'giant', 'girl', 'girlfriend', 'give', 'given', 'giving', 'go', 'goal', 'god', 'godzilla', 'going', 'gone', 'good', 'gore', 'got', 'government', 'grace', 'graphic', 'great', 'greatest', 'green', 'ground', 'group', 'guard', 'guess', 'gun', 'guy', 'hair', 'half', 'hall', 'hand', 'handle', 'hank', 'happen', 'happened', 'happens', 'happy', 'hard', 'hardly', 'harry', 'hate', 'head', 'hear', 'heard', 'heart', 'heavy', 'hell', 'help', 'henry', 'hero', 'high', 'highly', 'hilarious', 'hill', 'history', 'hit', 'hold', 'hole', 'hollywood', 'home', 'hong', 'hope', 'horrible', 'horror', 'horse', 'hot', 'hotel', 'hour', 'house', 'however', 'huge', 'human', 'humanity', 'humor', 'humorous', 'hundred', 'husband', 'ice', 'idea', 'ii', 'image', 'imagination', 'imagine', 'immediately', 'impact', 'important', 'impossible', 'impression', 'impressive', 'include', 'includes', 'including', 'incredible', 'incredibly', 'indeed', 'individual', 'industry', 'information', 'innocent', 'inside', 'inspired', 'instance', 'instead', 'intelligence', 'intelligent', 'intended', 'interest', 'interested', 'interesting', 'intriguing', 'introduced', 'involved', 'involves', 'involving', 'island', 'issue', 'jack', 'jackie', 'jackson', 'james', 'jason', 'jean', 'jeff', 'jennifer', 'jerry', 'jim', 'job', 'joe', 'john', 'join', 'joke', 'jones', 'journey', 'jr', 'julia', 'julie', 'jump', 'keep', 'kelly', 'kept', 'kevin', 'key', 'kid', 'kill', 'killed', 'killer', 'killing', 'kind', 'king', 'kiss', 'knew', 'know', 'known', 'kong', 'la', 'lack', 'lady', 'lame', 'land', 'language', 'large', 'last', 'late', 'later', 'latest', 'latter', 'laugh', 'laughing', 'law', 'lawyer', 'le', 'lead', 'leader', 'leading', 'leaf', 'learn', 'learns', 'least', 'leave', 'leaving', 'led', 'lee', 'left', 'legend', 'length', 'let', 'level', 'lie', 'life', 'light', 'liked', 'likely', 'limited', 'line', 'liner', 'list', 'literally', 'little', 'live', 'living', 'local', 'long', 'longer', 'look', 'looked', 'looking', 'lose', 'lost', 'lot', 'loud', 'love', 'loved', 'lover', 'low', 'lucas', 'machine', 'mad', 'made', 'magic', 'main', 'major', 'make', 'making', 'male', 'man', 'manages', 'manner', 'many', 'mar', 'mark', 'marriage', 'married', 'martial', 'martin', 'mary', 'master', 'masterpiece', 'match', 'material', 'matrix', 'matter', 'matthew', 'max', 'may', 'maybe', 'mean', 'meaning', 'meant', 'medium', 'meet', 'member', 'memorable', 'memory', 'men', 'menace', 'mention', 'mentioned', 'merely', 'mess', 'message', 'michael', 'middle', 'might', 'mike', 'mile', 'military', 'million', 'mind', 'minor', 'minute', 'miss', 'missing', 'mission', 'mistake', 'mix', 'model', 'modern', 'moment', 'money', 'monster', 'month', 'mood', 'moral', 'mostly', 'mother', 'motion', 'mouth', 'move', 'moving', 'mr', 'much', 'murder', 'murphy', 'music', 'musical', 'must', 'mysterious', 'mystery', 'na', 'nafter', 'nall', 'nalso', 'nalthough', 'name', 'named', 'nan', 'nand', 'nanother', 'nat', 'natural', 'nature', 'nbecause', 'nboth', 'nbut', 'nby', 'ndespite', 'ndirector', 'ndon', 'near', 'nearly', 'necessary', 'need', 'needed', 'neither', 'neven', 'never', 'nevery', 'new', 'news', 'next', 'nfirst', 'nfor', 'nfrom', 'nhe', 'nher', 'nhere', 'nhis', 'nhow', 'nhowever', 'ni', 'nice', 'nicely', 'nick', 'nif', 'night', 'nin', 'ninstead', 'nit', 'njust', 'nlet', 'nlike', 'nmaybe', 'nmeanwhile', 'nmost', 'nmy', 'nno', 'nnot', 'nnow', 'nof', 'noh', 'non', 'nonce', 'none', 'normal', 'note', 'nothing', 'notice', 'novel', 'nowhere', 'nperhaps', 'nshe', 'nsince', 'nso', 'nsome', 'nstill', 'nsure', 'nthat', 'ntheir', 'nthen', 'nthere', 'nthese', 'nthey', 'nthis', 'nthough', 'nto', 'number', 'numerous', 'nunfortunately', 'nwe', 'nwell', 'nwhat', 'nwhen', 'nwhile', 'nwhy', 'nwith', 'nyes', 'nyou', 'obvious', 'obviously', 'occasionally', 'odd', 'offer', 'office', 'officer', 'often', 'oh', 'old', 'older', 'onto', 'open', 'opening', 'opinion', 'opportunity', 'opposite', 'order', 'original', 'oscar', 'others', 'otherwise', 'outside', 'overall', 'owner', 'pace', 'pain', 'pair', 'parent', 'park', 'parody', 'part', 'particular', 'particularly', 'partner', 'party', 'pas', 'past', 'pathetic', 'patient', 'patrick', 'paul', 'pay', 'people', 'perfect', 'perfectly', 'performance', 'performer', 'perhaps', 'period', 'person', 'personal', 'personality', 'peter', 'phantom', 'phone', 'physical', 'pick', 'picture', 'piece', 'place', 'plan', 'plane', 'planet', 'play', 'played', 'player', 'playing', 'pleasure', 'plenty', 'plot', 'point', 'pointless', 'police', 'political', 'poor', 'poorly', 'pop', 'popular', 'portrayal', 'possible', 'possibly', 'potential', 'power', 'powerful', 'predictable', 'premise', 'presence', 'present', 'presented', 'president', 'pretty', 'previous', 'price', 'princess', 'prison', 'prisoner', 'private', 'probably', 'problem', 'process', 'produced', 'producer', 'product', 'production', 'prof', 'professional', 'project', 'promise', 'protagonist', 'prove', 'provide', 'provides', 'public', 'pull', 'pulp', 'pure', 'purpose', 'put', 'quality', 'queen', 'question', 'quick', 'quickly', 'quite', 'race', 'rare', 'rarely', 'rate', 'rated', 'rather', 'rating', 'reach', 'read', 'reading', 'ready', 'real', 'realistic', 'reality', 'realize', 'realized', 'really', 'reason', 'recent', 'recently', 'recommend', 'record', 'red', 'reference', 'relationship', 'release', 'released', 'relief', 'remains', 'remake', 'remember', 'rescue', 'respect', 'rest', 'result', 'return', 'revenge', 'review', 'rich', 'richard', 'ride', 'ridiculous', 'right', 'rise', 'rising', 'road', 'robert', 'robin', 'rock', 'roger', 'role', 'romance', 'romantic', 'room', 'rule', 'run', 'running', 'rush', 'russell', 'ryan', 'sad', 'said', 'sam', 'satire', 'save', 'saving', 'saw', 'say', 'saying', 'scare', 'scary', 'scene', 'school', 'sci', 'science', 'scientist', 'score', 'scott', 'scream', 'screen', 'screenplay', 'screenwriter', 'script', 'sean', 'search', 'seat', 'second', 'secret', 'see', 'seeing', 'seem', 'seemed', 'seemingly', 'seems', 'seen', 'self', 'sense', 'sent', 'sequel', 'sequence', 'series', 'serious', 'seriously', 'set', 'setting', 'seven', 'several', 'sex', 'sexual', 'shakespeare', 'shallow', 'shame', 'share', 'ship', 'shock', 'shoot', 'shooting', 'short', 'shot', 'show', 'showing', 'shown', 'side', 'sight', 'sign', 'silent', 'silly', 'similar', 'simon', 'simple', 'simply', 'since', 'singer', 'single', 'sister', 'sit', 'sitting', 'situation', 'six', 'skill', 'slasher', 'slave', 'slightly', 'slow', 'slowly', 'small', 'smart', 'smile', 'smith', 'snake', 'social', 'society', 'soldier', 'solid', 'somehow', 'someone', 'something', 'sometimes', 'somewhat', 'somewhere', 'son', 'song', 'soon', 'sort', 'soul', 'sound', 'soundtrack', 'space', 'speak', 'special', 'specie', 'speech', 'speed', 'spend', 'spends', 'spent', 'spielberg', 'spirit', 'spot', 'stage', 'stand', 'standard', 'star', 'starring', 'start', 'state', 'station', 'stay', 'steal', 'step', 'stephen', 'steve', 'steven', 'stick', 'still', 'stone', 'stop', 'store', 'story', 'storyline', 'straight', 'strange', 'street', 'strike', 'strong', 'struggle', 'stuck', 'student', 'studio', 'stuff', 'stunning', 'stunt', 'stupid', 'style', 'sub', 'subject', 'subplot', 'subtle', 'success', 'successful', 'suddenly', 'suit', 'summer', 'superb', 'superior', 'support', 'supporting', 'suppose', 'supposed', 'sure', 'surprise', 'surprised', 'surprising', 'surprisingly', 'suspect', 'suspense', 'sweet', 'system', 'take', 'taken', 'taking', 'tale', 'talent', 'talented', 'talk', 'talking', 'tarantino', 'target', 'tarzan', 'task', 'teacher', 'team', 'technical', 'teen', 'teenage', 'teenager', 'television', 'tell', 'telling', 'ten', 'tension', 'term', 'terrible', 'terrific', 'test', 'thanks', 'theater', 'theme', 'thin', 'thing', 'think', 'thinking', 'third', 'thomas', 'though', 'thought', 'three', 'thrill', 'thriller', 'throughout', 'throw', 'thrown', 'thus', 'tim', 'titanic', 'title', 'today', 'together', 'told', 'tom', 'tone', 'took', 'top', 'total', 'totally', 'touch', 'touching', 'tough', 'toward', 'towards', 'town', 'toy', 'track', 'trailer', 'train', 'travel', 'travolta', 'treat', 'trek', 'trick', 'tried', 'trip', 'trooper', 'trouble', 'true', 'truly', 'truman', 'truth', 'try', 'trying', 'turn', 'turned', 'tv', 'twenty', 'twice', 'twist', 'two', 'type', 'typical', 'ultimately', 'understand', 'unfortunately', 'unfunny', 'unique', 'unlike', 'upon', 'us', 'use', 'used', 'using', 'usual', 'usually', 'value', 'vampire', 'van', 'various', 'vega', 'vehicle', 'version', 'victim', 'video', 'view', 'viewer', 'viewing', 'villain', 'violence', 'violent', 'virtually', 'vision', 'visit', 'visual', 'voice', 'wait', 'waiting', 'walk', 'wall', 'want', 'wanted', 'war', 'warrior', 'waste', 'wasted', 'watch', 'watching', 'water', 'way', 'weak', 'weapon', 'wear', 'wedding', 'week', 'weird', 'well', 'went', 'west', 'whatever', 'whether', 'white', 'whole', 'whose', 'wide', 'wife', 'wild', 'william', 'williams', 'willis', 'win', 'wind', 'window', 'winner', 'winning', 'wise', 'wish', 'wit', 'witch', 'within', 'without', 'witty', 'woman', 'wonder', 'wonderful', 'woo', 'wood', 'woody', 'word', 'work', 'worked', 'worker', 'working', 'world', 'worse', 'worst', 'worth', 'worthy', 'would', 'write', 'writer', 'writing', 'written', 'wrong', 'wrote', 'year', 'yes', 'yet', 'york', 'young', 'younger']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidfconverter = TfidfTransformer()\n",
    "X2 = tfidfconverter.fit_transform(X1).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TfidfTransformer turns the count array earlier into an array of weights for each unique word based on the document:\n",
    "- TF-IDF weight is a weight often used in information retrieval and text mining\n",
    "- This weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus\n",
    "- The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus\n",
    "- Variations of the tf-idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.07599077 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.20052791 0.         0.         0.         0.04253558 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.03153765 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.07461927 0.         0.         0.\n",
      " 0.         0.06965278 0.         0.         0.         0.\n",
      " 0.06772453 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.03756588 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.06633847 0.07983243\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.07763989 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.1321944  0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.0523999  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.03216359 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.07770022 0.         0.07538821 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.0660972\n",
      " 0.         0.         0.03373273 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.07885446\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.06001851 0.         0.         0.         0.03912609 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.06733614\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.52958963 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.03233802 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.06438116 0.\n",
      " 0.         0.         0.         0.         0.06291102 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.05200282 0.         0.         0.         0.06767596 0.16056766\n",
      " 0.         0.         0.         0.         0.         0.07808419\n",
      " 0.05655962 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.10067936 0.         0.0564945\n",
      " 0.         0.         0.         0.04031382 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.05774422\n",
      " 0.         0.07933701 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.04353999 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.05714248 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.0726378\n",
      " 0.         0.         0.         0.         0.06036738 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.0599323  0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.1384448  0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.02630558 0.0518352\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.06352689 0.         0.039783\n",
      " 0.06449076 0.         0.         0.05059806 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.04273898 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.05933981 0.         0.04646309 0.07387087 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.08988821 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.08113206 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.05346174 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.05578692\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.06562387 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.03366986 0.05766793 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.05984649 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.08644485 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.10289977 0.         0.0674647  0.         0.05942331 0.\n",
      " 0.         0.         0.         0.15198154 0.         0.\n",
      " 0.         0.         0.         0.         0.03992894 0.\n",
      " 0.03544673 0.07405389 0.04948713 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.07346671 0.         0.         0.\n",
      " 0.05272392 0.         0.         0.07043589 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.05194678 0.         0.         0.         0.06384266\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.06241294 0.         0.\n",
      " 0.04110836 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.0504426  0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.06183235 0.05904896 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.07442545 0.         0.         0.         0.         0.\n",
      " 0.06658291 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.09156438 0.\n",
      " 0.         0.         0.         0.         0.03218289 0.\n",
      " 0.15151403 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.07213342 0.         0.         0.\n",
      " 0.08427067 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.07578774 0.\n",
      " 0.         0.         0.06967072 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.08679637 0.         0.         0.05257271\n",
      " 0.07442545 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.05491333 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.03563181 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.04318748 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.06438116\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.06241294 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.03424347\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.08196777 0.         0.         0.         0.\n",
      " 0.21690009 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.07492591 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.08453559\n",
      " 0.         0.         0.         0.         0.         0.12533443\n",
      " 0.10982666 0.09284296 0.06352689 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.05257271\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.04437711 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.13148216 0.\n",
      " 0.         0.         0.         0.06504868 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.03838346 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.03932357 0.         0.05537862\n",
      " 0.         0.         0.         0.         0.         0.06438116\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.05065017 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.03206738\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.03259291 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.05868504 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.06720846 0.0518909\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.05286435 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.05984649 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.05900954 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.06670634\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.07578774\n",
      " 0.         0.         0.         0.         0.         0.03138903\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.04343353 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.16234008 0.06126937 0.         0.05585585 0.         0.02914348\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.03146319 0.         0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(threshold=np.inf)\n",
    "print(X2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that the word that is highly weighted in this first document happens to be at the 18th index from the above print. Looking into it further, the word at the 18th index is 'action' which occurs alot in that review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names()[18])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(X2, y, [i for i in range(2000)], test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the indices are randomly shuffled, the X_test that we have is filled with randomly selected items from the documents array but we can tell which ones are where since we saved the shuffled indices. For example, the **first item in X_test** is the **405th item in documents**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[405, 1190, 1132, 731, 1754, 1178, 1533, 1303, 1857, 18, 1266, 1543, 249, 191, 721, 1896, 452, 1947, 1544, 1205, 1905, 1235, 799, 1594, 1091, 1357, 479, 1148, 511, 1931, 1257, 1105, 1938, 1750, 1058, 427, 1025, 156, 491, 995, 27, 1326, 1239, 17, 1622, 519, 361, 289, 1790, 1135, 1549, 1327, 76, 579, 279, 935, 475, 1786, 1023, 1556, 842, 1522, 1108, 1369, 47, 794, 918, 1220, 506, 353, 276, 881, 1835, 1703, 333, 1616, 135, 182, 1761, 1501, 930, 107, 390, 1165, 1698, 262, 1732, 260, 487, 303, 53, 1015, 148, 1442, 1580, 1999, 1927, 80, 1854, 264, 1979, 1273, 538, 1529, 37, 896, 1129, 1617, 1276, 878, 386, 1820, 977, 1074, 1542, 648, 1922, 77, 689, 9, 861, 6, 1083, 161, 1600, 1110, 962, 1385, 775, 1174, 823, 145, 240, 215, 1889, 465, 987, 1238, 760, 252, 1633, 781, 1451, 572, 85, 634, 317, 118, 1784, 1876, 443, 1900, 516, 1646, 30, 713, 254, 900, 587, 1317, 1887, 1811, 1121, 1334, 1918, 906, 1635, 1858, 796, 1076, 522, 1411, 124, 187, 517, 1945, 686, 342, 1787, 200, 1421, 1511, 1954, 233, 1620, 1344, 1295, 887, 229, 1893, 1426, 682, 440, 557, 899, 1363, 1981, 384, 1158, 1604, 1018, 655, 436, 921, 1340, 1781, 1610, 1621, 453, 1361, 294, 744, 310, 1188, 220, 376, 1406, 259, 1341, 658, 402, 706, 814, 530, 564, 1666, 1955, 1798, 459, 1834, 654, 34, 1808, 1985, 1717, 1147, 1124, 1557, 629, 1356, 512, 597, 999, 955, 610, 1299, 458, 385, 892, 175, 1991, 1715, 1756, 1694, 1744, 61, 1867, 1632, 1271, 227, 1093, 14, 521, 1150, 501, 1203, 1246, 1957, 1618, 1785, 574, 746, 839, 609, 546, 1005, 745, 223, 1677, 1039, 1492, 1196, 1507, 152, 1003, 688, 773, 677, 1367, 55, 141, 1644, 1256, 202, 933, 575, 982, 723, 1487, 801, 1452, 1044, 1279, 674, 1200, 1901, 393, 217, 1704, 1713, 39, 1088, 66, 1034, 1642, 981, 206, 399, 1471, 1695, 651, 1480, 5, 58, 1775, 1528, 898, 1440, 1358, 92, 1217, 1741, 1654, 1050, 831, 278, 793, 133, 1476, 144, 1906, 1386, 758, 1127, 795, 1926, 1724, 19, 1502, 880, 54, 1892, 1437, 789, 943, 1189, 1281, 1929, 1, 762, 1460, 113, 1222, 1051, 438, 1478, 82, 379, 1212, 1446, 326, 811, 322, 716, 1355, 1579, 461, 425, 1101, 122, 883, 446, 48, 1197, 866, 1374, 1263, 503, 1366, 1752, 298, 52, 1859, 1337, 1336, 638, 360, 1810, 1743, 563]\n"
     ]
    }
   ],
   "source": [
    "print(indices_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
       "                       n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "classifier.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note to self**: Draw out a single movie review tree here (if the review contains >3 'bad' words, it's negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest example:\n",
    "\n",
    "<img align=\"left\" src=https://miro.medium.com/max/900/1*EFBVZvHEIoMdYHjvAZg8Zg.gif width=\"450\" /> \n",
    "<img align=\"left\" src=https://static.javatpoint.com/tutorial/machine-learning/images/random-forest-algorithm2.png width=\"450\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pros of random forest:\n",
    "- Great predictive performance for binary classification\n",
    "- They provide a reliable feature importance estimate\n",
    "- They offer efficient estimates of the test error without incurring the cost of repeated model training associated with cross-validation\n",
    "- Handles thousands of input variables without variable deletion\n",
    "\n",
    "Cons of random forest:\n",
    "- An ensemble model is inherently less interpretable than an individual decision tree\n",
    "- Training a large number of deep trees can have high computational costs (but can be parallelized) and use a lot of memory\n",
    "- Predictions are slower, which may create challenges for applications\n",
    "\n",
    "More considerations: \n",
    "- https://github.com/TayariAmine/ML_cheat_sheet/wiki/Random-forest-Pros-and-Cons\n",
    "- https://www.oreilly.com/library/view/hands-on-machine-learning/9781789346411/e17de38e-421e-4577-afc3-efdd4e02a468.xhtml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1600"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'this is crap , but , honestly , what older american audience is going to be able to resist seeing jack lemmon and james garner as bicker- ing ex-presidents ? \\nespecially when their supporting players in- clude dan aykroyd as the current commander in chief , lauren bacall as a former first lady , and john heard as the dan quayle-ish vice president . \\nyup , you\\'re talkin\\' pre-sold property here and , for warner brothers , the perfect fit into their now-ritual grumpy old men holiday slot . \\nfor the non-discriminating viewer , my fellow americans is fine . \\nthe raw star power alone will have audiences applauding this atrocious political- thriller road-comedy . \\n ( they did in mine , heaven help us . ) \\nfor the rest of us , the movie is immediately tiresome . \\nthe tone is terrible and the banter is worse . \\nforget wit-- lemmon and garner merely exchange profanities through most of the movie . \\n ( has anyone counted the number of first penis references ? ) \\nsure , some of the bits are absurdly funny , including a men\\'s room macarena joke , the appearance of an elvis impersonator on a trainload of tarheels , and an all dorothy marching band performing \" over the rainbow \" at a gay men\\'s march . \\nthe get there from here , though , you have to submit to one of the most offensively overbearing musical scores of all time . \\njudas priest , is there a single moment of silence in this film ? \\neven the dialogue gets drowned out . \\nwhat a waste . \\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[405]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[180  28]\n",
      " [ 30 162]]\n",
      "\n",
      "ACCURACY: 0.855\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(\"\\nACCURACY: {}\".format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=https://glassboxmedicine.files.wordpress.com/2019/02/confusion-matrix.png width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.87      0.86       208\n",
      "           1       0.85      0.84      0.85       192\n",
      "\n",
      "    accuracy                           0.85       400\n",
      "   macro avg       0.85      0.85      0.85       400\n",
      "weighted avg       0.85      0.85      0.85       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 1266 10\n"
     ]
    }
   ],
   "source": [
    "# Finding False Positives (actual = 0, pred = 1)\n",
    "for i in range(len(y_pred)): \n",
    "    if (y_pred[i]==1 and y[indices_test[i]]!=y_pred[i]):\n",
    "        print(y[indices_test[i]], y_pred[i], indices_test[i], i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did some digging to find the first false positive (index 1266 in 'documents' and 10th predictions in 'pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the camera zoom in incredibly close nit focus on the closed eye of person presumed dead nall of sudden with thunderous sound effect and an eerie sounding musical pulse his eyelid open nthe camera zoom in incredibly close nit focus on one of the character desperately running for her life nthen with thunderous sound effect and an eerie sounding musical pulse she bump into the killer nthe camera zoom in incredibly close nit focus on door from where seemingly strange sound emanate nall of sudden with thunderous sound effect and an eerie sounding musical pulse the door swing open to reveal what inside nclose up thrill shot such a these seemed to elicit more laughter from the audience rather than genuine fear which is pretty good sign that you re not watching the thriller that the director had originally envisioned nrather you adopt the opinion that the character are so goofy and so unaware of the trouble that they re in that you stop caring about who life who dy and where the story go ninstead you become more interested in how the character get killed off nthe four young teenager who are the potential sacrificial lamb for this movie are helen and barry and julie and ray two couple and dear friend to one another nduring the summer just after high school graduation they take fateful drive down dark mountain highway nsuddenly without warning their car hit someone whom the teen presume wa killed a result of that collision nfearing jail time and possible manslaughter charge if they confess they instead decide to dump the body into the ocean and make pact never to discus the episode again na year go by and thing among the four friend begin to change nrelationships fizzle future dream crumble and attitude change nbut they can not forget what they did nand apparently neither can someone else nsomeone begin to send letter to all four of the teen with the frightening message know what you did last summer nthe letter writer figure in rubber slicker wielding large metal hook soon make his presence known and begin to hunt down the four teen nit now up to the four of them to try to figure out who the killer is before it too late nwhile this movie ha some nice visually eerie effect too much of it impact had to be generated with extreme close ups while the level of suspense wa choppy at best nthere is so much more that this movie could have been but decided to give u nothing but cheap thrill nit could have been clever whodunit but discovering who ultimately wa the killer made me just shrug my shoulder nit could have explored the changed friendship after year of adulthood but seemed to only include that aspect to add 15 more minute to the film nit could have given u smarter character who knew that anytime you re alone and you see shadowy figure moving about you don walk towards them yelling hello nonce you yell hello you can expect an extreme close up thunderous sound effect and an eerie sounding musical pulse nit difficult to be in suspense when you know what about to happen nthe only redeeming aspect of the movie is that now have neat idea for halloween costume but m not sure if the local costume store sell rubber slicker and hook nthe horror that experienced from this film is realizing that actually went to see it\n"
     ]
    }
   ],
   "source": [
    "print(documents[1266])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(y[1266])\n",
    "print(y_pred[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text_classifier', 'wb') as picklefile:\n",
    "    pickle.dump(classifier,picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text_classifier', 'rb') as training_model:\n",
    "    model = pickle.load(training_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[180  28]\n",
      " [ 30 162]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.87      0.86       208\n",
      "           1       0.85      0.84      0.85       192\n",
      "\n",
      "    accuracy                           0.85       400\n",
      "   macro avg       0.85      0.85      0.85       400\n",
      "weighted avg       0.85      0.85      0.85       400\n",
      "\n",
      "0.855\n"
     ]
    }
   ],
   "source": [
    "y_pred2 = model.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred2))\n",
    "print(classification_report(y_test, y_pred2))\n",
    "print(accuracy_score(y_test, y_pred2)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try It Yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.datasets import load_files\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "movie_data = load_files(r\"../review_polarity/txt_sentoken\")\n",
    "X, y = movie_data.data, movie_data.target\n",
    "documents = []\n",
    "\n",
    "new_X_test = ['I loved this movie so much!', 'This movie was bad and had terrible actors']\n",
    "count = len(new_X_test)\n",
    "new_X_test.extend(X)\n",
    "\n",
    "# Preprocessing functions\n",
    "def preprocessing(X):\n",
    "\n",
    "    stemmer = WordNetLemmatizer()\n",
    "\n",
    "    for sen in range(0, len(X)):\n",
    "        # Remove all the special characters\n",
    "        document = re.sub(r'\\W', ' ', str(X[sen]))\n",
    "\n",
    "        # remove all single characters\n",
    "        document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "\n",
    "        # Remove single characters from the start\n",
    "        document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "\n",
    "        # Substituting multiple spaces with single space\n",
    "        document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "\n",
    "        # Removing prefixed 'b'\n",
    "        document = re.sub(r'^b\\s+', '', document)\n",
    "\n",
    "        # Converting to Lowercase\n",
    "        document = document.lower()\n",
    "\n",
    "        # Lemmatization\n",
    "        document = document.split()\n",
    "\n",
    "        document = [stemmer.lemmatize(word) for word in document]\n",
    "        document = ' '.join(document)\n",
    "\n",
    "        documents.append(document)\n",
    "\n",
    "    vectorizer = CountVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "    X1 = vectorizer.fit_transform(documents).toarray()\n",
    "\n",
    "    tfidfconverter = TfidfTransformer()\n",
    "    X2 = tfidfconverter.fit_transform(X1).toarray()\n",
    "    \n",
    "    return X2\n",
    "\n",
    "new_X2 = preprocessing(new_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review: i loved this movie so much, pred: [1]\n",
      "review: this movie wa bad and had terrible actor, pred: [0]\n"
     ]
    }
   ],
   "source": [
    "with open('text_classifier', 'rb') as training_model:\n",
    "    model = pickle.load(training_model)\n",
    "\n",
    "for i in range(count):\n",
    "    review = documents[i]\n",
    "    prediction = model.predict([new_X2[i]])\n",
    "    print(\"review: {}, pred: {}\".format(review, prediction))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
